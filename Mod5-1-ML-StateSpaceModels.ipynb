{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","colab":{"name":"Mod5-1-ML-StateSpaceModels.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-7Ss-KkDBzMu"},"source":["\n","# <font color=#770000>ICPE 639 Introduction to Machine Learning </font>\n","\n","## ------ With Energy Applications\n","\n","<p> &#169; 2021: Xiaoning Qian </p>\n","\n","[Homepage](http://xqian37.github.io/)\n","\n","**<font color=blue>[Note]</font>** This is currently a work in progress, will be updated as the material is tested in the class room.\n","\n","All material open source under a Creative Commons license and free for use in non-commercial applications.\n","\n","Source material used under the Creative Commons Attribution-NonCommercial 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n"]},{"cell_type":"markdown","metadata":{"id":"8ifbVhVQe9Kz"},"source":["## State-Space Models\n","\n","### Model formulations\n","\n","State-space models aim at modeling dynamic systems stemming from differential or difference equations depending on the continuous-time or discrete-time signal representations. Here we provide an introduction for simple **linear dynamic systems**: \n","\\begin{eqnarray*}\n","\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\phi_t; \\\\\n","\\mathbf{y}_t = M \\mathbf{x}_t + \\epsilon_t, \n","\\end{eqnarray*}\n","where $\\mathbf{x}_t$ is the *state* vector for the dynamic signal (random process) of interest while $\\mathbf{y}_t$ denotes the measurements that are observed; $A$ governs the state dynamics while $M$ characterizes the measurement process; $\\phi_t$ and $\\epsilon_t$ correspond to system and measurement noise respectively. Different model setups can be used depending on different state space and model assumptions. The argubaly most well-known model is with Gaussian assumptions to all the involved variables, leading to the well-known **Kalman filtering**. With discrete-time signal representations, this state-space model can give the traditional **Auto-Regressive (AR)** and **Moving Average (MA)**  or **ARIMA (AR integrated MA)** model setups if we do not consider \"hidden states\", for example, using the first equation only to model observed time series. \n","\n","**<font color=blue>[Note]</font>** The above noise terms can be made to incorporate potential external input by setting up the corresponding distribution models, which can be important for corresponding signal processing and control problems, as well as **reinforcement learning**, for example to incorporate potential baseline **level**, **trend**, **seasonality**, **periodicity**, etc. in time series analysis, including energy applications (e.g. assuming $\\phi_t \\sim \\mathcal{N}(B\\mathbf{u}_t, \\Sigma_\\phi)$). But here we introduce the math based on the above simple model setup. "]},{"cell_type":"markdown","metadata":{"id":"kef9GIl2RLUg"},"source":["### Representive problem formulations\n","\n","The following lists three most represetnative problems using state-space models: \n","\n","1. **Learning (Model parameter estimation/inference)**: Given observed data (complete or partial), how to estimate unknown model parameters? \n","2. **Filtering**: Given observed measurements $Y_t = \\{\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_t\\}$, how to infer the current state $\\mathbf{x}_t$? \n","3. **Smoothing**: Given observed measurements $Y_T = \\{\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_T\\}$, how to infer a previous state $\\mathbf{x}_t$ ($t<T$)?  \n","4. **Preciction (Forecasting)**: Given observed measurements $Y_t = \\{\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_t\\}$, how to predict the state at the next time point $\\mathbf{x}_{t+1}$? "]},{"cell_type":"markdown","metadata":{"id":"Q6kdk2nw6Qh-"},"source":["### Probabilistic graphical models (PGM)\n","\n","Although we will not get deep into PGM in this course, we will introduce the basics in the context of state-space models here. In particular, we will focus on the linear state-space models with Gaussian random variables. We note that there are many reinvented wheels in different applications and therefore different names/terms referring to the same thing. For example, in linear dynamic systems (LDS), hidden Markov models (HMM), time series analysis, etc. We will introduce things based on traditional signal processing research, in particular related to **Kalman filtering**. \n","\n","To solve all the four problems mentioned earlier, the critical starting point is to understand the model and derive the corresponding **likelihood** function of the corresponding unknown(s) given the observed data. "]},{"cell_type":"markdown","metadata":{"id":"XSnMS4_X7tjp"},"source":["### Kalman filtering\n","\n","Let's revisit the model with the Gaussian assumptions this time (*linear Gaussian state-space model*---**LG-SSM**): \n","\\begin{eqnarray*}\n","\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\phi_t; \\\\\n","\\mathbf{y}_t = M \\mathbf{x}_t + \\epsilon_t, \n","\\end{eqnarray*}\n","where $\\phi_t \\sim \\mathcal{N}(0, \\Sigma_\\phi)$ and $\\epsilon_t \\sim \\mathcal{N}(0, \\Sigma_\\epsilon)$. Note that we make the *stationarity* assumptions here as $A$, $M$, and the mean and covariance parameters do not change with time. As all the variables are Gaussian and the model is linear, with some basic algebraic manupilation, we can compute the corresponding probability functions and thereafter derive the corresponding **likelihood* function given the specific problem of interest, let it be learning, filtering, smoothing, or prediction. \n","\n","We focus on Kalman filtering in this course and more comprehensive introduction on other problems can be found in several machine learning books, including PML (https://probml.github.io/pml-book/) and BRML (http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage)."]},{"cell_type":"markdown","metadata":{"id":"xlLNWVDyQiAt"},"source":["For **filtering**, we are interested in estimate the current state $\\mathbf{x}_t$ given the observed measurements so far  $Y_t = \\{\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_t\\}$. In probabilistic terms, we would like to derive: \n","$$p(\\mathbf{x}_t|Y_t).$$\n","We can see actually, this probability function can be considered as the likelihood of the current state $\\mathbf{x}_t$ given the oberved data $Y_t$ and it is a fundamental identity in state-space models. For example, once you know $p(\\mathbf{x}_t|Y_t)$, the **prediction** problem can be easily solved too: \n","$$p(\\mathbf{x}_{t+1}|Y_t) = \\int_{\\mathbf{x}_t}p(\\mathbf{x}_{t+1}, \\mathbf{x}_t|Y_t) d\\mathbf{x}_t = \\int_{\\mathbf{x}_t}p(\\mathbf{x}_{t+1} | \\mathbf{x}_t) p(\\mathbf{x}_t|Y_t)d\\mathbf{x}_t $$\n","based on the state-space model setup with the corresponding (conditional) dependency structures. Note that \n","$$p(\\mathbf{x}_{t+1} | \\mathbf{x}_t) \\sim \\mathcal{N}(A \\mathbf{x}_{t}, \\Sigma_\\phi)$$\n","based on the model. "]},{"cell_type":"markdown","metadata":{"id":"jJjYVf8AUEG_"},"source":["In fact, in this LG-SSM, $p(\\mathbf{x}_t|Y_t)$ is also Gaussian probability density function (PDF), we can write: \n","$$p(\\mathbf{x}_t|Y_t)\\sim \\mathcal{N}(\\mu_t, \\Sigma_t),$$\n","that can be characterized the mean $\\mu_t$ and covariance $\\Sigma_t$. The above is known as **moment representation**. \n","\n","**<font color=red>[Quiz/Homwork]</font>** Is $p(\\mathbf{x}_{t+1}|Y_t)$ Gaussian? Can you derive $p(\\mathbf{x}_{t+1}|Y_t)$ as a function of $\\mu_t$, $\\Sigma_t$, $A$, and $\\Sigma_\\phi$? (**Solution**: $A\\mu_t$, $A\\Sigma_t A^T + \\Sigma_\\phi$)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kSwbsdzS0zmy"},"source":["Now, we find out what should be $\\mu_t$ and $\\Sigma_t$ by deriving $p(\\mathbf{x}_t|Y_t)$ by the Bayes rule: \n","$$p(\\mathbf{x}_t|Y_t) = \\frac{ p(\\mathbf{x}_t|Y_{t-1}))p(\\mathbf{y}_t|\\mathbf{x}_t) }{ p(\\mathbf{y}_t|Y_{t-1})}$$\n","again using the dependency structure and Gaussianity in LG-SSM. \n","\n","Note now: \n","$$p(\\mathbf{y}_t|\\mathbf{x}_t) \\sim \\mathcal{N}(M\\mathbf{x}_t, \\Sigma_\\epsilon);$$\n","$$p(\\mathbf{x}_t|Y_{t-1}) = \\mathcal{N}(A\\mu_{t-1}, A\\Sigma_{t-1}A^T + \\Sigma_\\phi). $$\n","\n","By completing the quadratic functional form in the exponent, we can get the following recursive updates: \n","$$\\mbox{Let}\\quad P = A\\Sigma_{t-1}A^T + \\Sigma_\\phi, \\quad\\mbox{then}$$\n","$$\\mu_t = A\\mu_{t-1} + PM^T(MPM^T + \\Sigma_\\epsilon)^{-1}(\\mathbf{y}_t-MA\\mu_{t-1})$$\n","$$\\Sigma_t = P - PM^T(MPM^T + \\Sigma_\\epsilon)^{-1}MP. $$"]},{"cell_type":"markdown","metadata":{"id":"PYmpSAcO3yrn"},"source":["Note that this essentially is the basic Kalman filtering implementation based on **forward algorithm**. The solution is indeed related to message passing or belief propagation algorithms in PGM. But in LG-SSM, we can see this iterative updating algorithm has closed-form updating solutions. For more general state-space models, more complicated solution algorithms have to be developed. \n","\n","Finally, for **smoothing**, we will need to essentially develop a **forward-backward algorithm** to compute the **moment representations* of the corresponding states. Again, the essense of the solution can be found in many reinvented wheels, for example *Viterbi algorithm* or many other dynamic programming solutions. \n","\n","**Learning** with only observed measurements $Y_T$ can be formulated and solved by **EM** algorithm, which again involves computing the corresponding likelihood of unknown model parameters given data. For the students who are interested in math derivation, the aforementioned two books can be good references. \n","\n","\n","\n","\n","**<font color=red>[Work to do]</font>** Need to find a hands-on here, energy applications would be nice... \n","\n","Do we need to do AR/MA with anomaly detection using Gaussianity? maybe not? \n","\n","For more flexible state-space models, Extended or Unscented Kalman Filters (EKF, UKF) were developed with other more flexible distribution models (as in more general **particle filtering**). The difficulty is in model inference, which involves challenging Bayesian iverse problems. \n","\n","The other extreme direction is to completely data-driven to develop **recurrent neural networks (RNN)**, which can be found in another module."]},{"cell_type":"markdown","metadata":{"id":"hIVRDfEQiewh"},"source":["## Reference\n","*Some materials in this section are adapted from several resources listed below:* \n","\n","- https://towardsdatascience.com/\n","- [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/)\n","- An Introduction to Statistical Learning : with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. New York: Springer, 2013.\n","- Open Machine Learning Course mlcourse.ai."]},{"cell_type":"markdown","metadata":{"id":"NQ2Cud9cEZEj"},"source":["# Questions? "]},{"cell_type":"code","metadata":{"id":"6N8DlOKOxbUk","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1619819380866,"user_tz":300,"elapsed":300,"user":{"displayName":"Xiaoning Qian","photoUrl":"","userId":"06431978792501680815"}},"outputId":"da67c7e0-a567-41d2-d639-3a924d9fdc5d"},"source":["Image(url= \"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\", width=100)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=\"100\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"bXkjGYbNe9K5"},"source":[""],"execution_count":null,"outputs":[]}]}