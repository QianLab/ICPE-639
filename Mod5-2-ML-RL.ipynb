{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7Ss-KkDBzMu"
   },
   "source": [
    "\n",
    "# <font color=#770000>ICPE 639 Introduction to Machine Learning </font>\n",
    "\n",
    "## ------ With Energy Applications\n",
    "\n",
    "<p> &#169; 2021: Xiaoning Qian </p>\n",
    "\n",
    "[Homepage](http://xqian37.github.io/)\n",
    "\n",
    "**<font color=blue>[Note]</font>** This is currently a work in progress, will be updated as the material is tested in the class room.\n",
    "\n",
    "All material open source under a Creative Commons license and free for use in non-commercial applications.\n",
    "\n",
    "Source material used under the Creative Commons Attribution-NonCommercial 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSpTU7H9DWl8"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "This section will provide a **very basic** introduction to Reinforcement Learning (RL): \n",
    "\n",
    "- [1 Markov Decision Process](#1-Markov-Decision-Process)\n",
    "- [2 Q Learning](#2-Q-Learning)\n",
    "- [3 Deep RL](#3-Deep-RL)\n",
    "- [4 Hands-on Exercise](#4-Exercise)\n",
    "- [Reference](#Reference)\n",
    "\n",
    "**<font color=blue>[Note]</font>**: Most of the materials here were based on the Microsoft free course: https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/README.md and https://towardsdatascience.com/understanding-the-markov-decision-process-mdp-8f838510f150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ifbVhVQe9Kz"
   },
   "source": [
    "## 1 Markov Decision Process\n",
    "\n",
    "RL is to make the *optimal* decision, with model or model-free for the underlying system, to achieve the maximum **reward**. \n",
    "\n",
    "### Basics\n",
    "\n",
    "RL is to make the *optimal* decision, with model or model-free for the underlying system, to achieve the maximum **reward**. For model-based decision making (control theory), Markov Decision Process (MDP) can serve as the foundation. The underlying dynamic model of MDP can be related to the state-space model that we just introduced. \n",
    "\n",
    "MDP often involves the following critical components: \n",
    "1. **State** in a state space $\\mathcal{X}$;\n",
    "2. **Action** in the action space $\\mathcal{A}$;\n",
    "3. **Transition** $\\mathbf{P}$ governing the state dynamics (state-space models): $\\mathbf{P}: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{X}$ (deterministic) or $\\mathbf{P}: \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X} \\rightarrow [0, 1]$ (probabilistic);\n",
    "4. **Reward** $\\mathbf{R}$ modeling the expected reward to take a certain action to reach different states: $\\mathbf{R}: \\mathcal{X} \\times \\mathcal{A} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "With these four elements, we can model how an **agent** may interact with the **environment** based on $\\mathbf{P}$. The goal of RL is to derive a good **policy** to achieve the best reward based on $\\mathbf{R}$. If we have the model $\\mathbf{P}$ and $\\mathbf{R}$, we are with model-based RL. Otherwise, we are doing model-free RL, a famous one of which is **Q-learning**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8ovH9wOc1Yl"
   },
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/max/6000/1*p4JW6ibYcdmaeXVB_klivA.png\" alt=\"MDP1\">\n",
    "\n",
    "<center>A Schematic for an artificial MDP</center>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1750/1*QLeo3MikUeGvNyVjl5mqoA.png\" alt=\"MDP2\">\n",
    "\n",
    "<center>A Schematic for a Robot MDP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmnfUeC3fNVv"
   },
   "source": [
    "\n",
    "### Markov Property\n",
    "\n",
    "MDP can be considered as a **Markov Chain** with actions and assigned rewards. The introductory materials of MDP often starts with the finite state space and action space. Given the current state, the future is often assumed to be **conditionally independent** with the past, known as **Markov property**: \n",
    "$$\\mathbf{P}(x_{t+1} | x_t, a_t; \\mbox{ the past state-action history }) = \\mathbf{P}(x_{t+1} | x_{t}, a_t).$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kef9GIl2RLUg"
   },
   "source": [
    "### MDP problem formulations\n",
    "\n",
    "The goal is to derive a **control policy** to specific which action to take given the state at specific time: \n",
    "$$\\pi(a|x; t) = P(A_t = a | X_t = x). $$\n",
    "\n",
    "The control policy can be either **deterministic** or **probabilistic**. If the policy is not dependent on time, $\\pi(a|x; t) =\\pi(a |x)$, then it is **stationary**, which is often the case in many applications.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N69yxdSNlBaM"
   },
   "source": [
    "### Value Iteration\n",
    "\n",
    "#### Policy Value: \n",
    "\n",
    "The **value** of a given policy $\\pi(a|x)$ at $x$ is the **expected cumulative rewards** (often discounted) with the **discount factor** $\\gamma \\in (0, 1)$: \n",
    "$$V(x) = \\mathbb{E}_P[\\mathbf{R}(x_0=x, a_0, x_1) + \\gamma \\mathbf{R}(x_1, a_1, x_2) + \\ldots + \\gamma^t \\mathbf{R}(x_t=x, a_t, x_{t+1}) + \\ldots|\\pi]$$\n",
    "\n",
    "By divide and conquer (an important trick for **dynamic programming**): \n",
    "$$V(x) = \\mathbb{E}_{x_1}[\\mathbb{E}_P[\\mathbf{R}(x_0=x, a_0, x_1) + \\gamma V(x_1)]]$$\n",
    "\n",
    "#### Optimal value and optimal action: \n",
    "\n",
    "The logic of dynamic programming: To achieve the optimality of the whole sequence, each sub-sequence shall be also optimal. This leads to the famous **Bellman Optimality Equation**: \n",
    "\n",
    "$$V_{opt}(x) = \\max_{a\\in\\mathcal{A}}{\\sum_{x_1} \\mathbf{P}(x_1|x_0=x, a_0=a)(\\mathbf{R}(x_0=x, a_0, x_1) + \\gamma V_{opt}(x_1) )}$$\n",
    "\n",
    "Assume that the iteration converges to a **stationary** policy (discounting). This immediate gives the **value iteration algorithm**: \n",
    "\n",
    "$$V_{k+1} = \\max_{a\\in\\mathcal{A}}{\\sum_{x'} \\mathbf{P}(x'|x, a)(\\mathbf{R}(x, a, x') + \\gamma V_{k}(x') )}.$$\n",
    "\n",
    "$V_{k} \\rightarrow V_{opt}$ for stationary control policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nFQHQsBR5LQ"
   },
   "source": [
    "### State-Action (Q) Value\n",
    "\n",
    "The Q value of a given policy $\\pi(a|x)$ at $x$ with action $a$ is the expected cumulative rewards with the discount factor $\\gamma$:\n",
    "$$Q(x, a) = \\mathbb{E}_{x_1}[\\mathbb{E}_P[\\mathbf{R}(x_0=x, a_0=a, x_1) + \\gamma \\max_{a_1} Q(x_1, a_1)]].$$\n",
    "\n",
    "It has the same recursive structure. \n",
    "\n",
    "### Q value iteraction algorithm: \n",
    "\n",
    "We can now derive the **optimal policy**: \n",
    "$$\\pi_{opt}(a|x) = \\arg\\max_a Q_{opt}(x,a),$$\n",
    "and \n",
    "$$Q_{k+1}(x,a) = \\sum_{x'} \\mathbf{P}(x'|x, a)(\\mathbf{R}(x, a, x') + \\gamma \\max_{a_1} Q_k(x_1, a_1))$$\n",
    "(Q-value iteraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bq7RFmJVCgZ"
   },
   "source": [
    "### Policy Iteration\n",
    "\n",
    "We can also derive the **policy iteration** algorithm with the same dynamic programming trick: \n",
    "\n",
    "$$\\pi_k(a|x) = \\arg\\max_a{\\sum_{x'} \\mathbf{P}(x'|x, a)(\\mathbf{R}(x, a, x') + \\gamma V_{\\pi_k}(x') )},$$\n",
    "where $V_{\\pi_k}(x')$ is evaluated value based on the previous policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zu-Sv_RWD8k"
   },
   "source": [
    "## 2 Q Learning\n",
    "\n",
    "Q Learning is a model-free RL method when we do not have the model for transition $\\mathbf{P}$ or reward $\\mathbf{R}$. Note that to be exact, RL often referrs to the model-free situations where you have to derive the policy based on the interaction between the agent and environment through experience/observations, *in an end-to-end fashion*. The policy $\\pi(a|x)$ is learned only driven by the observed reward $r(x,a)$, for example as \"relayed experience\". \n",
    "\n",
    "$$ \\mathcal{D} = \\{(x, a, r, x')_t\\}_{t=0}^{T} \\rightarrow Q_{opt}(x, a) \\rightarrow \\pi_{opt}(a|x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuCO0-82xECQ"
   },
   "source": [
    "### Q Learning Algorithm\n",
    "\n",
    "1. Initialize Q-Table **$Q$** with equal numbers for all states and actions\n",
    "2. Set **learning rate** $\\alpha\\leftarrow 1$\n",
    "3. Repeat simulation many times\n",
    "\n",
    "   A. Start at random position\n",
    "\n",
    "   B. Repeat\n",
    "\n",
    "      1. Select an action $a$ at state $x$\n",
    "      2. Exectute action by moving to a new state $x'$\n",
    "      3. If we encounter end-of-game condition, or total reward is too small - exit simulation\n",
    "      4. Compute reward $r$ at the new state\n",
    "      5. Update Q-Function according to Bellman equation: $$Q(x,a)\\leftarrow (1-\\alpha)Q(x,a)+\\alpha(r+\\gamma\\max_{a'}Q(x',a'))$$\n",
    "      6. $x\\leftarrow x'$\n",
    "      7. Update total reward and decrease $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jrn1Pd3j4k1w"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_bCbsAyyf-h"
   },
   "source": [
    "### Exploration vs. Exploitation\n",
    "\n",
    "At the step **3.B.2**, if random policy is adopted, it is simply **exploring** the state and action space randomly, which can be time-consuming to converge to the optimal policy. \n",
    "\n",
    "**$\\epsilon$-greedy policy**: With $1-\\epsilon$ probabilty to take the action that gives the best Q value (**exploitation**). \n",
    "\n",
    "**<font color=red>Homework 3</font>** Implement Q Learning with $\\epsilon$-greedy policy in the following example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxt1TzsmyzdI"
   },
   "source": [
    "### Temporal Difference Learning\n",
    "\n",
    "Similarly, from the value iteration algorithm, we can derive the corresponding Temporal Difference Learning (TD Learning): \n",
    "\n",
    "$$V(x)\\leftarrow (1-\\alpha)V(x)+\\alpha(r+\\gamma V(x'))$$\n",
    "\n",
    "This is simply updating the running average (**incremental learning**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u9tBCwj1z_k"
   },
   "source": [
    "<font color=red>example</font> Please check: \n",
    "\n",
    "https://github.com/ageron/handson-ml/blob/master/16_reinforcement_learning.ipynb\n",
    "\n",
    "https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/solution/notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJlgGeXp3Joo"
   },
   "source": [
    "### Q Learning with Function Approximation\n",
    "\n",
    "Instead of estimating Q value based on running average as above, it may be beneficial to have a model or function to **approximate** the Q value so that we may further improve the convergence properties if we can quickly learn a reasonably good function approximation: \n",
    "\n",
    "$$Q(x, a) \\leftarrow \\tilde{Q}(x, a; \\theta)$$\n",
    "\n",
    "In the above algorithm, instead of directly estimating $Q$, we can update the model parameter $\\theta$ (related to **SGD**): \n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha_t (r_t +\\gamma \\max_{a'}\\tilde{Q}(x',a'; \\theta) - \\tilde{Q}(x,a; \\theta) )\\nabla_\\theta \\tilde{Q}(x, a; \\theta) $$\n",
    "\n",
    "Clearly, different function approximation can be adopted here as in other machine learning problems. This leads to the recent development of **Deep RL**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0azZyhR-Nqi"
   },
   "source": [
    "### Policy Gradient\n",
    "\n",
    "Another branch is to directly approximate the policy and learn it end-to-end: \n",
    "$$\\pi(a|x) \\leftarrow \\tilde{\\pi}(a|x; \\theta)$$\n",
    "\n",
    "We will not discuss here this in detail. One of important challenges to address is to derive good gradient estimate of $\\nabla_\\theta \\tilde{\\pi}(a|x; \\theta)$. One of famous estimation method is **REINFORCE**. There have been many new methods to improve policy gradient by either better modeling $\\tilde{\\pi}(a|x; \\theta)$ or better gradient estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8pug_cL5cx1"
   },
   "source": [
    "## 3 Deep Q Learning\n",
    "\n",
    "<img src=\"https://pylessons.com/static/images/Reinforcement-learning/03_CartPole-reinforcement-learning/Dueling-Q-architecture.jpg\" alt=\"DQL\" size=500>\n",
    "\n",
    "<center>A Schematic for deep Q learning</center>\n",
    "\n",
    "\n",
    "<img src=\"https://developer.ibm.com/developer/default/articles/machine-learning-and-gaming/images/Figure5.png\" alt=\"DQL\" size=500>\n",
    "\n",
    "<center>A Schematic for deep Q learning for policy gradient in Gaming </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6kdk2nw6Qh-"
   },
   "source": [
    "Note that once the approximating neural network, MLP, CNN, RNN, or GNN based on the state and action space, is specified, the learning is the same as all the other machine learning problems. \n",
    "\n",
    "Here, also note that we are not restricted ourselves to finite state or action space any more!\n",
    "\n",
    "There have been also many challenges to theoretically analyze DQN performances and empirically further improve DQN. Two main issues with vanilla DQN is the **bias** problem of overestimating Q values in noisy environment and the **moving target** problem due to using the same DQN to evaluate and choose actions. There have been recent efforts to address these problems, including **Prioritized experience replay** and **dueling/double DQN**, etc. \n",
    "\n",
    "\n",
    "More tutorials can be found at: \n",
    "\n",
    "https://github.com/pythonlessons/Reinforcement_Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYmpSAcO3yrn"
   },
   "source": [
    "\n",
    "\n",
    "**<font color=red>[Work to do]</font>** Need to find a hands-on here, energy applications would be nice... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIVRDfEQiewh"
   },
   "source": [
    "## Reference\n",
    "*Some materials in this section are adapted from several resources listed below:* \n",
    "\n",
    "- https://towardsdatascience.com/\n",
    "- An Introduction to Statistical Learning : with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. New York: Springer, 2013.\n",
    "- Open Machine Learning Course mlcourse.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ2Cud9cEZEj"
   },
   "source": [
    "# Questions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1619819380866,
     "user": {
      "displayName": "Xiaoning Qian",
      "photoUrl": "",
      "userId": "06431978792501680815"
     },
     "user_tz": 300
    },
    "id": "6N8DlOKOxbUk",
    "outputId": "da67c7e0-a567-41d2-d639-3a924d9fdc5d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" width=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\", width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXkjGYbNe9K5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "Mod5-2-ML-RL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
